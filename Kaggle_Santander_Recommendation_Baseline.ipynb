{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular 데이터 다루는 머신 러닝 파이프라인\n",
    "1. 데이터 전처리\n",
    "2. 피쳐 엔지니어일\n",
    "3. 머신 러닝 모델 학습\n",
    "4. 테스트 데이터예측 및 캐글 업로드 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 전처리\n",
    "1. 제품변수의 결측값을 0으로 대체\n",
    "    - 제품 보유 여부에 대한 정보가 없으면, 해당 제품은 없다고 가정\n",
    "2. 훈련 데이터와 테스트 데이터 초합\n",
    "    - 날짜 변수로 구분 가능 ( fecha_dato )\n",
    "    - 동일한 24개의 고객 변수를 공유하며, 테스트 데이터에 없는 24개의 제품 변수를 0으로 채운다\n",
    "3. 범주형, 수치형 데이터 전처리\n",
    "    - 범주형은 .factorize() 통해 Label Encoding \n",
    "    - 데이터 타입이 Object로 표시된 수치형 데이터 ( e.g) age )는 .unique()를 통해 특이값들을 대체, 제거 후 정수형으로 전환\n",
    "4. 모델 학습에 사용할 변수이름을 features 리스트에 미리 담자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle First job is start.. \n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-62bf7e10781b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# 데이터를 불러온다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Kaggle First job is start.. '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtrn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\jaesang\\\\kaggle\\\\train_ver2.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train set load finished..'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jaesang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jaesang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jaesang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1034\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jaesang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._try_int64\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "np.random.seed(2018)\n",
    "\n",
    "# 데이터를 불러온다.\n",
    "print('Kaggle First job is start.. ')\n",
    "trn = pd.read_csv('C:\\\\Users\\\\jaesang\\\\kaggle\\\\train_ver2.csv')\n",
    "print('train set load finished..')\n",
    "\n",
    "tst = pd.read_csv('C:\\\\Users\\\\jaesang\\\\kaggle\\\\test_ver2.csv')\n",
    "print('test set load finished..')\n",
    "\n",
    "## 데이터 전처리 ##\n",
    "\n",
    "# 제품 변수를 별도로 저장해 놓는다.\n",
    "prods = trn.columns[24:].tolist()\n",
    "\n",
    "# 제품 변수 결측값을 미리 0으로 대체한다.\n",
    "trn[prods] = trn[prods].fillna(0.0).astype(np.int8)\n",
    "\n",
    "# 24개 제품 중 하나도 보유하지 않는 고객 데이터를 제거한다.\n",
    "no_product = trn[prods].sum(axis=1) == 0\n",
    "trn = trn[~no_product]\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터를 통합한다. 테스트 데이터에 없는 제품 변수는 0으로 채운다.\n",
    "for col in trn.columns[24:]:\n",
    "    tst[col] = 0\n",
    "df = pd.concat([trn, tst], axis=0)\n",
    "\n",
    "# 학습에 사용할 변수를 담는 list이다.\n",
    "features = []\n",
    "\n",
    "# 범주형 변수를 .factorize() 함수를 통해 label encoding한다.\n",
    "categorical_cols = ['ind_empleado', 'pais_residencia', 'sexo', 'tiprel_1mes', 'indresi', 'indext', 'conyuemp', 'canal_entrada', 'indfall', 'tipodom', 'nomprov', 'segmento']\n",
    "for col in categorical_cols:\n",
    "    df[col], _ = df[col].factorize(na_sentinel=-99)\n",
    "features += categorical_cols\n",
    "\n",
    "# 수치형 변수의 특이값과 결측값을 -99로 대체하고, 정수형으로 변환한다.\n",
    "df['age'].replace(' NA', -99, inplace=True)\n",
    "df['age'] = df['age'].astype(np.int8)\n",
    "\n",
    "df['antiguedad'].replace('     NA', -99, inplace=True)\n",
    "df['antiguedad'] = df['antiguedad'].astype(np.int8)\n",
    "\n",
    "df['renta'].replace('         NA', -99, inplace=True)\n",
    "df['renta'].fillna(-99, inplace=True)\n",
    "df['renta'] = df['renta'].astype(float).astype(np.int8)\n",
    "\n",
    "df['indrel_1mes'].replace('P', 5, inplace=True)\n",
    "df['indrel_1mes'].fillna(-99, inplace=True)\n",
    "df['indrel_1mes'] = df['indrel_1mes'].astype(float).astype(np.int8)\n",
    "\n",
    "# 학습에 사용할 수치형 변수를 features에 추구한다.\n",
    "features += ['age','antiguedad','renta','ind_nuevo','indrel','indrel_1mes','ind_actividad_cliente']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 피쳐 엔지니어링\n",
    "\n",
    "#### 모델에 학습할 파생 변수 생성\n",
    "\n",
    "##### Baseline에서는 24개의 고객 변수 + 4개의 날짜 변수 기반 파생변수 + 24개의 Lag-1 변수 사용 \n",
    "\n",
    "1. 4개의 날짜 변수 생성\n",
    "    - fecha_alta 변수 = 고객이 첫 계약을 맺은 날짜\n",
    "    - ult_fec_cli_1t 변수 = 고객이 마지막으로 1등급이었던 날짜\n",
    "    - 두개의 변수에서 각각 연도와 월 정보추출\n",
    "    > - 두개의 변수 간의 차이값을 파생변수 생성도 가능\n",
    "    > - 졸업식, 방학등 특별한 날짜까지의 거리르 수치형으로 변수로도 사용 가능 \n",
    "\n",
    "2. 결측값은 임시로 -99로 대체하자\n",
    "    - 사이킷런에서 제공하는 ML 모델은 결측값을 입력값으로 받지않고 에러를 발생\n",
    "    - Xgboost 모델은 결측값도 정상 입력값으로 받아, 데이터가 결측되었다는 것도 하나의 정보로 인식하고 모델 학습에 활용 \n",
    "    - 이번 Baseline은 -99로 하자\n",
    "    \n",
    "3. 시계열 데이터는 고객의 과거 데이터 기반 다양한 파생 변수 만들 수 있음\n",
    "    - 고객의 나이가 최근 3개월 동안 변동있는가 ( 생일이 지났음을 의미 ) - 이진 변수로 생성\n",
    "    - 한달 전에 구매한 제품에 정보를 변수로 사용\n",
    "    - 최근 평균 월급 계산도 가능\n",
    "    \n",
    "4. N개월 전에 금융 제품을 보유하고 있었는지 여부를 나타내는 Lag 변수가 좋은 파생 변수로 작용! \n",
    "    - 24개의 금융 제품 변수에 대하여 1개월, 2개월, 3개월 전 보유 여부를 변수로 활용\n",
    "    - baseline에서는 1개월 전만 사용 ( lag-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (피쳐 엔지니어링) 두 날짜 변수에서 연도와 월 정보를 추출한다.\n",
    "df['fecha_alta_month'] = df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\n",
    "df['fecha_alta_year'] = df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n",
    "features += ['fecha_alta_month', 'fecha_alta_year']\n",
    "\n",
    "df['ult_fec_cli_1t_month'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\n",
    "df['ult_fec_cli_1t_year'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n",
    "features += ['ult_fec_cli_1t_month', 'ult_fec_cli_1t_year']\n",
    "\n",
    "# 그 외 변수의 결측값은 모두 -99로 대체한다.\n",
    "df.fillna(-99, inplace=True)\n",
    "\n",
    "# (피쳐 엔지니어링) lag-1 데이터를 생성한다.\n",
    "# 코드 2-12와 유사한 코드 흐름이다.\n",
    "\n",
    "# 날짜를 숫자로 변환하는 함수이다. 2015-01-28은 1, 2016-06-28은 18로 변환된다\n",
    "def date_to_int(str_date):\n",
    "    Y, M, D = [int(a) for a in str_date.strip().split(\"-\")] \n",
    "    int_date = (int(Y) - 2015) * 12 + int(M)\n",
    "    return int_date\n",
    "\n",
    "# 날짜를 숫자로 변환하여 int_date에 저장한다\n",
    "df['int_date'] = df['fecha_dato'].map(date_to_int).astype(np.int8)\n",
    "\n",
    "# 데이터를 복사하고, int_date 날짜에 1을 더하여 lag를 생성한다. 변수명에 _prev를 추가한다.\n",
    "df_lag = df.copy()\n",
    "df_lag.columns = [col + '_prev' if col not in ['ncodpers', 'int_date'] else col for col in df.columns ]\n",
    "df_lag['int_date'] += 1\n",
    "\n",
    "# 원본 데이터와 lag 데이터를 ncodper와 int_date 기준으로 합친다. Lag 데이터의 int_date는 1 밀려 있기 때문에, 저번 달의 제품 정보가 삽입된다.\n",
    "df_trn = df.merge(df_lag, on=['ncodpers','int_date'], how='left')\n",
    "\n",
    "# 메모리 효율을 위해 불필요한 변수를 메모리에서 제거한다\n",
    "del df, df_lag\n",
    "\n",
    "# 저번 달의 제품 정보가 존재하지 않을 경우를 대비하여 0으로 대체한다.\n",
    "for prod in prods:\n",
    "    prev = prod + '_prev'\n",
    "    df_trn[prev].fillna(0, inplace=True)\n",
    "df_trn.fillna(-99, inplace=True)\n",
    "\n",
    "# lag-1 변수를 추가한다.\n",
    "features += [feature + '_prev' for feature in features]\n",
    "features += [prod + '_prev' for prod in prods]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  머신러링 모델 학습\n",
    "\n",
    "#### 교차검증\n",
    "> 경진대회에서 좋은 성적을얻기 위해 가장 중요한 것 ! \n",
    "> 캐글은 하루에 최대 5개의 예측 결과만 제출할 수 있고, 결과는 Public 리더보드에점수 공개\n",
    "> 즉, 하루에 5번 밖에 확인할 수 없기에,다양한 실험을 제출로는 힘들 수 있다\n",
    "> 그래서 !! 올바른 교차검증 방법으로 제한없이 아이디어를 실험하고, 성능 개선 여부를 확인해보자\n",
    "\n",
    "\n",
    "- 훈련 셋 = 2015-01-28 ~ 2016-05-28, 총 1년 6개월치 데이터  \n",
    "- 테스트 셋 = 2016-06-28, 미래 데이터\n",
    "- 교차검증에서는 최신 날짜 2016-05-28를 검증 데이터로분리하고, 나머지는 훈련셋으로 사용\n",
    "> baseline은 간소화하기 위해 2016-01-28~2016-04-28, 총 4개월 훈련 / 2016-05-28을 검증으로 사용\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 모델 학습\n",
    "# 학습을 위하여 데이터를 훈련, 테스트용으로 분리한다.\n",
    "# 학습에는 2016-01-28 ~ 2016-04-28 데이터만 사용하고, 검증에는 2016-05-28 데이터를 사용한다.\n",
    "use_dates = ['2016-01-28', '2016-02-28', '2016-03-28', '2016-04-28', '2016-05-28']\n",
    "trn = df_trn[df_trn['fecha_dato'].isin(use_dates)]\n",
    "tst = df_trn[df_trn['fecha_dato'] == '2016-06-28']\n",
    "del df_trn\n",
    "\n",
    "# 훈련 데이터에서 신규 구매 건수만 추출한다.\n",
    "X = []\n",
    "Y = []\n",
    "for i, prod in enumerate(prods):\n",
    "    prev = prod + '_prev'\n",
    "    prX = trn[(trn[prod] == 1) & (trn[prev] == 0)]\n",
    "    prY = np.zeros(prX.shape[0], dtype=np.int8) + i\n",
    "    X.append(prX)\n",
    "    Y.append(prY)\n",
    "XY = pd.concat(X)\n",
    "Y = np.hstack(Y)\n",
    "XY['y'] = Y\n",
    "\n",
    "# 훈련, 검증 데이터로 분리한다. \n",
    "vld_date = '2016-05-28'\n",
    "XY_trn = XY[XY['fecha_dato'] != vld_date]\n",
    "XY_vld = XY[XY['fecha_dato'] == vld_date]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델\n",
    "\n",
    "##### XGBoost 모델 \n",
    "\n",
    "- max_depth\n",
    "> 트리 모델의 최대 깊이 = 깊을수록 복잡한 모델이지만 오버피팅 원인이 될 수 있음\n",
    "\n",
    "- eta\n",
    "> - learning_rate와 같은 개념 \n",
    "> - 0~1사이의 값으로 너무 낮으면 학습이 잘 안될 수 있으며, 너무 크면 학습이 느려짐\n",
    "\n",
    "- colsample-bytree\n",
    "> -트리를 생성할 때, 훈련 데이터에서 변수를 샘플링해주는 비율 \n",
    "> - 모든 트리는 전체 변수의 일부만 학습하여 서로의 약점을 보완해주는 모델 \n",
    "> - 보통 0.6~0.9\n",
    "\n",
    "- colsmaple_bylevel\n",
    "> - 트리의 레벨 별로 훈련 데이터 변수를 샘플링해주는 비율\n",
    "> - 보통 0.6~0.9\n",
    "\n",
    "* 참고\n",
    "\n",
    "> - 머신러닝을 처음하는 사람들은 모델의 최적화된 하이퍼파라미터찾는데 많은 시간 투자.. ( = 튜닝 작업 )\n",
    "> - 물론 이 것도 좋지만, 시간 투자 대비효율을 생각하면 피처 엔지니어링에 더 투자하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# XGBoost 모델 parameter를 설정한다.\n",
    "param = {\n",
    "    'booster': 'gbtree',\n",
    "    'max_depth': 8,\n",
    "    'nthread': 4,\n",
    "    'num_class': len(prods),\n",
    "    'objective': 'multi:softprob',\n",
    "    'silent': 1,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'eta': 0.1,\n",
    "    'min_child_weight': 10,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'colsample_bylevel': 0.9,\n",
    "    'seed': 2018,\n",
    "    }\n",
    "\n",
    "# 훈련, 검증 데이터를 XGBoost 형태로 변환한다.\n",
    "X_trn = XY_trn.as_matrix(columns=features)\n",
    "Y_trn = XY_trn.as_matrix(columns=['y'])\n",
    "dtrn = xgb.DMatrix(X_trn, label=Y_trn, feature_names=features)\n",
    "\n",
    "X_vld = XY_vld.as_matrix(columns=features)\n",
    "Y_vld = XY_vld.as_matrix(columns=['y'])\n",
    "dvld = xgb.DMatrix(X_vld, label=Y_vld, feature_names=features)\n",
    "\n",
    "# XGBoost 모델을 훈련 데이터로 학습한다!\n",
    "watch_list = [(dtrn, 'train'), (dvld, 'eval')]\n",
    "model = xgb.train(param, dtrn, num_boost_round=1000, evals=watch_list, early_stopping_rounds=20)\n",
    "\n",
    "# 학습한 모델을 저장한다.\n",
    "\n",
    "import pickle\n",
    "pickle.dump(model, open(\"'C:\\\\Users\\\\jaesang\\\\kaggle\\\\model\\\\xgb.baseline.pkl\", \"wb\"))\n",
    "best_ntree_limit = model.best_ntree_limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 교차검증\n",
    "\n",
    "- 교차검증은 평가척도인 MAP@7 사용하여 성능 수준을 확인\n",
    "> 경진대회에서 사용하는 평가 척도를 꼭 사용하자\n",
    "\n",
    "- MAP@7 평가 척도는 최고 점수가 데이터에 따라 변동할 수 있다\n",
    "- Baseline의 검증데이터에서는  MAP@7 최고 점수는 0.042663\n",
    "- 1보다 낮은 이유는 검증 데이터의 모든 고객이 신규 구매를 하지 않기 때문\n",
    "> - 100명의 고객 중 10명만이 신규구매를 헀다고 가정하면 그 10명을 정확히해도 결국 10%의 MAP를 받음 \n",
    "> - 그러므로 MAP@7 최고 점수를 감안해서 성능 평가하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP@7 평가 척도를 위한 준비작업이다.\n",
    "# 고객 식별 번호를 추출한다.\n",
    "vld = trn[trn['fecha_dato'] == vld_date]\n",
    "ncodpers_vld = vld.as_matrix(columns=['ncodpers'])\n",
    "# 검증 데이터에서 신규 구매를 구한다.\n",
    "for prod in prods:\n",
    "    prev = prod + '_prev'\n",
    "    padd = prod + '_add'\n",
    "    vld[padd] = vld[prod] - vld[prev]    \n",
    "add_vld = vld.as_matrix(columns=[prod + '_add' for prod in prods])\n",
    "add_vld_list = [list() for i in range(len(ncodpers_vld))]\n",
    "\n",
    "# 고객별 신규 구매 정답 값을 add_vld_list에 저장하고, 총 count를 count_vld에 저장한다.\n",
    "count_vld = 0\n",
    "for ncodper in range(len(ncodpers_vld)):\n",
    "    for prod in range(len(prods)):\n",
    "        if add_vld[ncodper, prod] > 0:\n",
    "            add_vld_list[ncodper].append(prod)\n",
    "            count_vld += 1\n",
    "\n",
    "# 검증 데이터에서 얻을 수 있는 MAP@7 최고점을 미리 구한다. (0.042663)\n",
    "print(mapk(add_vld_list, add_vld_list, 7, 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 검증 데이터에 대한 예측 값을 구한다.\n",
    "X_vld = vld.as_matrix(columns=features)\n",
    "Y_vld = vld.as_matrix(columns=['y'])\n",
    "dvld = xgb.DMatrix(X_vld, label=Y_vld, feature_names=features)\n",
    "preds_vld = model.predict(dvld, ntree_limit=best_ntree_limit)\n",
    "\n",
    "# 저번 달에 보유한 제품은 신규 구매가 불가하기 때문에, 확률값에서 미리 1을 빼준다\n",
    "preds_vld = preds_vld - vld.as_matrix(columns=[prod + '_prev' for prod in prods])\n",
    "\n",
    "# 검증 데이터 예측 상위 7개를 추출한다.\n",
    "result_vld = []\n",
    "for ncodper, pred in zip(ncodpers_vld, preds_vld):\n",
    "    y_prods = [(y,p,ip) for y,p,ip in zip(pred, prods, range(len(prods)))]\n",
    "    y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
    "    result_vld.append([ip for y,p,ip in y_prods])\n",
    "    \n",
    "# 검증 데이터에서의 MAP@7 점수를 구한다. (0.036466)\n",
    "print(mapk(add_vld_list, result_vld, 7, 0.0))\n",
    "\n",
    "# XGBoost 모델을 전체 훈련 데이터로 재학습한다!\n",
    "X_all = XY.as_matrix(columns=features)\n",
    "Y_all = XY.as_matrix(columns=['y'])\n",
    "dall = xgb.DMatrix(X_all, label=Y_all, feature_names=features)\n",
    "watch_list = [(dall, 'train')]\n",
    "# 트리 개수를 늘어난 데이터 양만큼 비례해서 증가한다.\n",
    "best_ntree_limit = int(best_ntree_limit * (len(XY_trn) + len(XY_vld)) / len(XY_trn))\n",
    "# XGBoost 모델 재학습!\n",
    "model = xgb.train(param, dall, num_boost_round=best_ntree_limit, evals=watch_list)\n",
    "\n",
    "# 변수 중요도를 출력해본다. 예상하던 변수가 상위로 올라와 있는가?\n",
    "print(\"Feature importance:\")\n",
    "for kv in sorted([(k,v) for k,v in model.get_fscore().items()], key=lambda kv: kv[1], reverse=True):\n",
    "    print(kv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Baseline모델은 MAP@ 0.036466 \n",
    "- 검증 데이터 최고 점수가 0.042663\n",
    "- 정확도는 (0.036466 / 0.042663 ) =0.85로 약 85%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.  테스트 데이터 예측 및 캐글 업로드\n",
    "\n",
    "- 교차검증에서 소중한 훈련 데이터의 일부를 도려내어 검증 데이터로 사용\n",
    "- 테스트 데이터에 좋은 성능을 내기 위하여, 훈련, 검증 데이터를 합친 전체 데이터에 대하여 다시 XGBoost 모델을 학습\n",
    "- XGBoost 모델 파라미터는 교차 검증에서 찾아낸 최적의 파라미터로 사용하되,\n",
    "- XGBoost에서 사용하는 트리 개수를 늘어난 검증 데이터만큼증가\n",
    "\n",
    "- 모델에서 변수 중요도 출력 ( XGBoost의 get_fscore() \n",
    "- 업로드\n",
    "\n",
    "### 끝!! 맺음말\n",
    "\n",
    "- Baseline 모델을 통해 데이터에 대한 이해도 깊어짐\n",
    "- 머신러닝 파이프라인이 정상적으로 동작하는 거 확인 \n",
    "- 이제 성능 개선하자!!\n",
    "\n",
    "- 다시 강조하고자하는 내용은 '모델 튜닝'보다는 '피처 엔지니어링'에 많은 시간 투자하자\n",
    "- 데이터에 대해서 심도 있게 고민하고 다양한 아이디어를 구현하고 실험하는 과정을 수행하면 , 많은 것을 얻을 수 잇을 것!\n",
    "\n",
    "### Baseline 모델 요약\n",
    "\n",
    "- 제공된 전체 훈련 데이터 중, 2016년도 데이터만을 학습에 사용 \n",
    "- 파생 변수는 2개의 날짜 데이터의 연도, 월 추출\n",
    "- 24개의 제품변수에 대한 lag-1변수만 사용 \n",
    "\n",
    "- XGBoost 모델 학습 과정에서 MAP@7를 직접 사용할 수 없음\n",
    "- XGBoost는 mlogloss를 통해 학습하였지만, 모델 파라미터를 선정하는 있어서는 \n",
    "- 자체 구현한 mapk()을 통하여 검증 데이터에 MAP@7 점수 사용\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
